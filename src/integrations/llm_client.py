from __future__ import annotations

import json
import os
import logging
from typing import Any, Dict, List, Optional, Sequence, Type, TypeVar, Union

import instructor
from instructor import Instructor, Provider
from openai.types.chat import (
    ChatCompletionSystemMessageParam,
    ChatCompletionUserMessageParam,
)
from pydantic import BaseModel

from src.pipeline.video_profiles import VideoProfile
from src.schemas.recipe import (
    Asset,
    MusicPlan,
    NarrativePlan,
    Scene,
    SceneBlueprint,
    SegmentGroupingPlan,
    VideoRecipe,
    VideoRecipeMetadata,
    VideoRecipeSceneChunk,
)

T = TypeVar("T", bound=BaseModel)

MessageParam = Union[
    ChatCompletionSystemMessageParam,
    ChatCompletionUserMessageParam,
]
MessageList = Sequence[MessageParam]

logger = logging.getLogger(__name__)

DEFAULT_CHANNEL_BRAND = "Isso tem explica√ß√£o"
CHANNEL_BRAND = (os.getenv("CHANNEL_BRAND_NAME") or DEFAULT_CHANNEL_BRAND).strip() or DEFAULT_CHANNEL_BRAND
BRAND_CONTEXT = (
    f"O canal '{CHANNEL_BRAND}' √© especializado em dark facts, curiosidades cient√≠ficas e mist√©rios inexplicados. "
    "Tom: curiosidade intensa + leve desconforto psicol√≥gico + fasc√≠nio cient√≠fico. "
    "P√∫blico: brasileiros de 25 a 65+ anos que adoram fatos perturbadores e conhecimento que desafia o senso comum."
)


def _resolve_provider(provider: str | Provider) -> Provider:
    if isinstance(provider, Provider):
        return provider
    provider_str = (provider or "").strip().lower()
    try:
        return Provider(provider_str)
    except ValueError as exc:
        raise ValueError(f"Unsupported LLM provider '{provider_str}'.") from exc


def create_instructor_client(
    *,
    provider_override: Optional[str | Provider] = None,
    model_override: Optional[str] = None,
) -> Instructor:
    timeout = float(os.getenv("LLM_TIMEOUT", "180"))

    provider_value = provider_override or os.getenv("LLM_PROVIDER") or Provider.OPENAI.value
    provider_enum = _resolve_provider(provider_value)

    if model_override is not None:
        llm = model_override.strip()
    else:
        llm = (os.getenv("LLM_MODEL") or "gpt-5").strip()

    mode = instructor.Mode.JSON if provider_enum is Provider.OPENAI else instructor.Mode.ANTHROPIC_JSON

    return instructor.from_provider(
        f"{provider_enum.value}/{llm}",
        mode=mode,
        timeout=timeout,
    )


class TypedLLMClient:
    def __init__(self):
        self._client = create_instructor_client()

    def _build_kwargs(
        self,
        *,
        response_model: Type[T],
        messages: MessageList,
        enable_reasoning: bool = False,
    ) -> Dict[str, Any]:
        kwargs: Dict[str, Any] = dict(
            messages=list(messages),
            response_model=response_model,
            max_retries=3,
        )

        provider = self._client.provider
        if provider is Provider.OPENAI:
            kwargs["response_format"] = {"type": "json_object"}
            kwargs["reasoning_effort"] = "high" if enable_reasoning else "medium"
        if provider is Provider.ANTHROPIC:
            kwargs["max_tokens"] = 30_000
            if enable_reasoning:
                try:
                    model_name = str(self._client.kwargs.get("model", ""))
                    if "sonnet" in model_name.lower() or "opus" in model_name.lower():
                        kwargs["thinking"] = {
                            "type": "enabled",
                            "budget_tokens": 10000
                        }
                        # Anthropic requires temperature to be exactly 1 when thinking mode is enabled.
                        kwargs["temperature"] = 1.0
                    else:
                        kwargs["temperature"] = 0.7
                except Exception:
                    kwargs["temperature"] = 0.7
            else:
                kwargs["temperature"] = 0.7
        return kwargs

    @staticmethod
    def _messages(system: str, user: str) -> List[MessageParam]:
        return [
            ChatCompletionSystemMessageParam(role="system", content=system),
            ChatCompletionUserMessageParam(role="user", content=user),
        ]

    @staticmethod
    def _chunk_segments(segments: List[dict], chunk_size: int) -> List[List[dict]]:
        chunk_size = max(1, chunk_size)
        return [
            segments[i : i + chunk_size]
            for i in range(0, len(segments), chunk_size)
        ]

    @staticmethod
    def _is_credit_exhausted_error(exc: Exception) -> bool:
        message = str(exc).lower()
        return "credit balance" in message or "insufficient credit" in message

    def _completion_typed(
        self,
        *,
        response_model: Type[T],
        messages: MessageList,
        enable_reasoning: bool = False,
    ) -> T:
        kwargs = self._build_kwargs(
            response_model=response_model,
            messages=messages,
            enable_reasoning=enable_reasoning,
        )
        try:
            return self._client.chat.completions.create(**kwargs)
        except Exception as exc:
            if self._client.provider is Provider.ANTHROPIC and self._is_credit_exhausted_error(exc):
                fallback_key = os.getenv("OPENAI_API_KEY")
                if fallback_key:
                    fallback_model = (os.getenv("LLM_FALLBACK_MODEL") or "gpt-5-mini").strip()
                    logger.warning(
                        "Anthropic credits exhausted; falling back to OpenAI provider with model '%s'.",
                        fallback_model,
                    )
                    self._client = create_instructor_client(
                        provider_override=Provider.OPENAI,
                        model_override=fallback_model,
                    )
                    kwargs = self._build_kwargs(
                        response_model=response_model,
                        messages=messages,
                        enable_reasoning=enable_reasoning,
                    )
                    return self._client.chat.completions.create(**kwargs)
                raise RuntimeError(
                    "Anthropic API retornou cr√©ditos insuficientes e nenhuma chave OPENAI_API_KEY est√° configurada. "
                    "Renove os cr√©ditos da Anthropic ou defina OPENAI_API_KEY (opcionalmente LLM_FALLBACK_MODEL) para habilitar o fallback autom√°tico."
                ) from exc
            raise

    @staticmethod
    def _cap_assets_for_duration(duration: float, requested_count: int) -> int:
        """Clamp the number of assets to a reasonable maximum for the scene duration."""
        safe_duration = max(0.1, duration)
        if safe_duration < 3.0:
            max_assets = 1
        elif safe_duration < 5.5:
            max_assets = 2
        elif safe_duration < 8.0:
            max_assets = 3
        else:
            max_assets = 4
        return max(1, min(requested_count, max_assets))

    def _generate_recipe_metadata(
        self,
        *,
        topic: str,
        narration_text: str,
        total_duration: float,
        audio_path: str,
        video_profile: VideoProfile | None,
    ) -> VideoRecipeMetadata:
        narration_preview = " ".join(narration_text.strip().split())
        if len(narration_preview) > 800:
            narration_preview = narration_preview[:800].rstrip() + "‚Ä¶"

        render_fps = video_profile.fps if video_profile else 60
        if video_profile and video_profile.orientation == "portrait":
            orientation_label = (
                f"vertical {video_profile.width}x{video_profile.height} (aspect_ratio {video_profile.aspect_ratio})"
            )
        elif video_profile:
            orientation_label = (
                f"horizontal {video_profile.width}x{video_profile.height} (aspect_ratio {video_profile.aspect_ratio})"
            )
        else:
            orientation_label = ""

        orientation_hint = (
            f"- Formato visual alvo: {orientation_label}.\n" if orientation_label else ""
        )

        system = (
            "Voc√™ √© um especialista VIRAL em YouTube Shorts/TikTok com 10+ anos otimizando algoritmos.\n"
            f"{BRAND_CONTEXT}\n\n"
            "OBJETIVO: Criar metadados que GARANTAM CTR 15%+ e AVD 85%+ atrav√©s de gatilhos psicol√≥gicos comprovados.\n\n"
            "üéØ F√ìRMULA DO T√çTULO VIRAL (40-60 chars):\n\n"
            "HOOKS TESTADOS E COMPROVADOS (CTR 20%+):\n"
            "‚Ä¢ Pergunta Direta + Emoji: 'Por que voc√™ [problema comum]? üò∞'\n"
            "‚Ä¢ Contradi√ß√£o + Emoji: 'ü§Ø [Cren√ßa comum] est√° ERRADO'\n"
            "‚Ä¢ Curiosity Gap + Emoji: '‚ö†Ô∏è O que [fen√¥meno] revela sobre voc√™'\n"
            "‚Ä¢ Identifica√ß√£o + Emoji: 'üò± Se voc√™ [a√ß√£o], isso acontece'\n"
            "‚Ä¢ Revela√ß√£o + Emoji: 'üî• O que [especialistas] escondem sobre [X]'\n"
            "‚Ä¢ Dark Hook + Emoji: '‚ò†Ô∏è A verdade sobre [X] que ningu√©m conta'\n\n"
            "REGRAS CR√çTICAS:\n"
            "- SEMPRE come√ßar com pergunta direta OU emoji relevante (n√£o ambos obrigatoriamente)\n"
            "- Use 1-2 palavras em CAPS para √™nfase estrat√©gica\n"
            "- N√∫meros espec√≠ficos quando aplic√°vel: '5x mais chance' > 'muito mais'\n"
            "- Criar identifica√ß√£o imediata: 'voc√™', 'seu c√©rebro', 'acontece com voc√™'\n"
            "- NUNCA use: 'Voc√™ sabia que...', 'Curiosidade sobre...', 'Fato interessante...', 'ALARME', 'PERSEGUE'\n"
            "- Evite verbos gen√©ricos tipo 'alarme', 'persegue' - seja espec√≠fico e direto\n\n"
            "üìù DESCRI√á√ÉO OTIMIZADA (100-150 palavras):\n\n"
            "ESTRUTURA COMPROVADA:\n"
            "[Linha 1]: Hook direto - afirma√ß√£o forte sobre o tema\n"
            "[Linha 2]: Linha em branco para respira√ß√£o visual\n"
            "[Linha 3-4]: Explica√ß√£o cient√≠fica/factual em linguagem acess√≠vel\n"
            "[Linha 5]: Linha em branco\n"
            "[Linha 6-7]: Revela√ß√£o impactante - 'O pior?' ou dados surpreendentes\n"
            "[Linha 8]: Linha em branco\n"
            "[Linha 9-10]: Teaser com n√∫meros/estudos espec√≠ficos\n"
            "[Linha 11]: Linha em branco\n"
            "[Linha 12]: CTA com emoji - 'Fica at√© o final' + promessa de valor\n"
            "[Linha 13]: Linha em branco\n"
            "[Linha 14]: CTA social - 'Marca aquela pessoa' com emoji üëá\n"
            "[Linha 15]: Linha em branco\n"
            "[Linha 16]: 5-7 hashtags relevantes\n\n"
            "REGRAS DE ESCRITA:\n"
            "- Par√°grafos curtos (1-2 linhas cada) com LINHAS EM BRANCO entre eles\n"
            "- Use CAPS apenas em 2-3 palavras-chave para impacto\n"
            "- Linguagem conversacional e direta\n"
            "- Dados espec√≠ficos > afirma√ß√µes vagas\n"
            "- Tom: 'fascinante mas acess√≠vel'\n\n"
            "HASHTAGS ESTRAT√âGICAS (5-7 no final da descri√ß√£o):\n"
            "PRIORIZA√á√ÉO:\n"
            "1. #Shorts (sempre primeira)\n"
            "2. Hashtag PRINCIPAL do tema (ex: #AcordarAs3h)\n"
            "3. 2-3 hashtags do nicho (ex: #Ins√¥nia #Ansiedade #Cortisol)\n"
            "4. 2-3 hashtags amplas (ex: #Sa√∫deMental #Ci√™nciaDoSono)\n"
            "Evite: #CuriosidadesBR #FatosCuriosos (muito gen√©ricas)\n\n"
            "üè∑Ô∏è TAGS ESTRAT√âGICAS (12-15 tags):\n"
            "MIX OBRIGAT√ìRIO:\n"
            "‚Ä¢ 2-3 tags longtail EXATAS do tema: frases que pessoas buscam\n"
            "  Exemplo: 'acordar √†s 3 da manh√£', 'por que acordo √†s 3h'\n"
            "‚Ä¢ 3-4 tags espec√≠ficas do nicho: palavras-chave principais\n"
            "  Exemplo: 'ins√¥nia', 'ansiedade', 'cortisol', 'sono'\n"
            "‚Ä¢ 3-4 tags m√©dias relacionadas: contexto do tema\n"
            "  Exemplo: 'sa√∫de mental', 'ci√™ncia do sono', 'acordar de madrugada'\n"
            "‚Ä¢ 2-3 tags amplas: alcance geral\n"
            "  Exemplo: 'shorts brasil', 'curiosidades', 'fatos cient√≠ficos'\n\n"
            "PRIORIZA√á√ÉO DE TAGS:\n"
            "- As 3-4 primeiras devem ser as mais espec√≠ficas (longtail)\n"
            "- Tags devem refletir termos REAIS de busca\n"
            "- Evite tags muito gen√©ricas tipo 'brasil', 'v√≠deo curto'\n"
            "- Inclua varia√ß√µes naturais: 'acordar √†s 3h' E 'acordar de madrugada'\n\n"
            "‚ö†Ô∏è REGRAS DE SEGURAN√áA YOUTUBE:\n"
            "‚úÖ PERMITIDO: Fatos cient√≠ficos educativos, fen√¥menos naturais, curiosidades psicol√≥gicas\n"
            "‚ùå PROIBIDO: Clickbait enganoso, sensacionalismo vazio, promessas n√£o cumpridas\n\n"
            "PSICOLOGIA DO CONTE√öDO:\n"
            "- Curiosidade informativa (n√£o sensacionalismo)\n"
            "- Identifica√ß√£o pessoal ('acontece com voc√™')\n"
            "- Valor educativo real + entretenimento\n"
            "- Tom: 'isso √© fascinante E voc√™ vai aprender algo'\n\n"
            "FORMATO: JSON `VideoRecipeMetadata` com todos campos otimizados para m√°ximo alcance viral."
        )

        user = (
            f"TEMA: {topic.strip()}\n"
            f"CANAL: {CHANNEL_BRAND}\n\n"
            f"NARRA√á√ÉO (preview):\n{narration_preview}\n\n"
            "ESPECIFICA√á√ïES T√âCNICAS:\n"
            f"- audio.path: {audio_path}\n"
            f"- audio.duration_sec: {total_duration:.3f}\n"
            f"- policy.render: adapter='editly', fps={render_fps}, audio_codec='aac', pixel_format='yuv420p', preset='ultrafast'\n"
            "- fade_in_sec: 0.2\n"
            "- fade_out_sec: 0.3\n"
            f"{orientation_hint}\n"
            "- language: 'pt-BR'\n\n"
            "CRIE METADADOS QUE:\n"
            "1. GARANTAM clique imediato (CTR 15%+)\n"
            "2. Usem gatilhos psicol√≥gicos dark mas YouTube-safe\n"
            "3. Prometam valor que o v√≠deo REALMENTE entrega\n"
            "4. Otimizem para busca E algoritmo simultaneamente\n"
            "5. Gerem compartilhamento org√¢nico ('voc√™ precisa ver isso')\n\n"
            "Retorne JSON v√°lido com metadados VIRAIS."
        )

        messages: List[MessageParam] = self._messages(system, user)
        metadata = self._completion_typed(
            response_model=VideoRecipeMetadata,
            messages=messages,
        )
        metadata.language = "pt-BR"
        metadata.audio.path = audio_path
        metadata.audio.duration_sec = round(total_duration, 3)
        if video_profile:
            try:
                metadata.policy.render.fps = video_profile.fps
            except Exception:
                pass
        return metadata

    def generate_music_prompt(
        self,
        topic: str,
        narration_text: str,
        duration_sec: float,
    ) -> MusicPlan:
        narration_compact = " ".join((narration_text or "").strip().split())
        if len(narration_compact) > 600:
            narration_compact = narration_compact[:600].rstrip() + "‚Ä¶"

        system = (
            "Voc√™ √© produtor musical especializado em trilhas VIRAIS para YouTube Shorts/TikTok.\n"
            f"{BRAND_CONTEXT}\n\n"
            "MISS√ÉO: Criar prompt para ElevenLabs Music que gere trilha HIPNOTIZANTE para m√°xima reten√ß√£o.\n\n"
            "üéµ F√ìRMULA DA TRILHA VIRAL (pesquisas 2024-2025):\n\n"
            "ESTRUTURA COMPROVADA (85%+ completion rate):\n"
            "1. IN√çCIO IMEDIATO (0-3s): Sem intro, direto no groove principal\n"
            "2. BUILD SUTIL (3-20s): Tens√£o crescente mas NUNCA drop pesado\n"
            "3. SUSTENTA√á√ÉO (20-40s): Energia constante com micro-varia√ß√µes\n"
            "4. LOOP PERFEITO (40-45s): Final conecta suavemente ao in√≠cio\n\n"
            "ELEMENTOS OBRIGAT√ìRIOS NO PROMPT:\n"
            "‚Ä¢ Ritmo/Pulse: SEMPRE incluir 'steady pulse', 'constant rhythm', 'hypnotic beat'\n"
            "‚Ä¢ In√≠cio: SEMPRE incluir 'start immediately with main groove, no intro'\n"
            "‚Ä¢ Background: SEMPRE incluir 'sits softly behind voice', 'supports narration'\n"
            "‚Ä¢ Atmosfera: Usar palavras emocionais, n√£o t√©cnicas\n\n"
            "VOCABUL√ÅRIO PARA DARK/CURIOSIDADES:\n"
            "‚úÖ USAR: mysterious, eerie, tense, dark ambient, unsettling undertone, psychological tension\n"
            "‚úÖ USAR: curious, discovery, scientific wonder, space-like, deep ocean vibes\n"
            "‚úÖ USAR: cinematic, atmospheric, moody, haunting but not horror\n"
            "‚ùå EVITAR: BPM, Hz, dB, seconds, duration, loud, aggressive, horror\n\n"
            "INSTRUMENTA√á√ÉO VIRAL:\n"
            "Dark content: deep bass, dark synth pads, subtle strings, atmospheric drones\n"
            "Curiosidades: soft bells, ethereal pads, light percussion, cosmic synths\n"
            "Tens√£o: rising strings, ticking clock, heartbeat rhythm, breath sounds\n\n"
            "ESTRUTURA DO PROMPT (100-250 chars):\n"
            "[Mood] + [Rhythm] + [Instruments] + [Atmosphere] + [Mixing]\n\n"
            "EXEMPLOS DE ALTA PERFORMANCE:\n"
            "'Dark mysterious atmosphere with steady hypnotic beat, deep bass and ethereal pads, "
            "psychological tension building, sits softly behind voice, start immediately'\n\n"
            "'Eerie scientific discovery vibe, constant subtle pulse, atmospheric drones and soft bells, "
            "unsettling but curious, supports narration without overpowering'\n\n"
            "FORMATO: JSON `MusicPlan` com campo `prompt` em INGL√äS."
        )

        user = (
            f"TEMA: {topic.strip()}\n\n"
            f"NARRA√á√ÉO (resumo):\n{narration_compact}\n\n"
            f"DURA√á√ÉO: {duration_sec:.2f}s\n\n"
            "Crie prompt (100-250 chars) para trilha que:\n"
            "1. HIPNOTIZE o espectador (ritmo constante viciante)\n"
            "2. Amplifique emo√ß√£o dark/curiosa sem dominar voz\n"
            "3. Comece IMEDIATAMENTE no groove principal\n"
            "4. Crie tens√£o psicol√≥gica sutil\n"
            "5. Loop perfeito para rewatches\n\n"
            "N√ÉO mencione dura√ß√£o, BPM ou termos t√©cnicos."
        )

        messages: List[MessageParam] = self._messages(system, user)
        return self._completion_typed(
            response_model=MusicPlan,
            messages=messages,
        )

    def generate_segment_grouping_plan(
        self,
        *,
        segments: List[dict],
        topic: str,
    ) -> SegmentGroupingPlan:
        segments_summary = []
        for seg in segments:
            idx = seg.get("index", 0)
            start = seg.get("start", 0.0)
            end = seg.get("end", 0.0)
            text = (seg.get("text", "") or "").strip()
            duration = round(end - start, 2)
            segments_summary.append(f"{idx}. [{start:.1f}-{end:.1f}s, {duration}s] \"{text}\"")

        segments_text = "\n".join(segments_summary)
        total_segments = len(segments)

        system = (
            "Voc√™ √© especialista em ritmo narrativo VIRAL para YouTube Shorts/TikTok.\n"
            f"{BRAND_CONTEXT}\n\n"
            "MISS√ÉO: Agrupar segments para criar ritmo HIPNOTIZANTE com mudan√ßas visuais a cada 2.5-3.5s.\n\n"
            "‚ö° CI√äNCIA DO RITMO VIRAL (dados 2024-2025):\n"
            "- Mudan√ßa visual a cada 2.5-3.5s = 35% mais reten√ß√£o\n"
            "- Grupos de 3-5s = ritmo ideal para n√£o cansar\n"
            "- Varia√ß√£o de dura√ß√£o mant√©m aten√ß√£o\n\n"
            "REGRAS DE AGRUPAMENTO VIRAL:\n\n"
            "1) SEMPRE AGRUPAR (obrigat√≥rio para fluxo):\n"
            "   A) Fragmentos < 2s: Muito curtos, quebram ritmo\n"
            "   B) Interjei√ß√µes isoladas: 'S√©rio?', 'Nossa!', 'Olha s√≥'\n"
            "   C) Conectivos: 'Mas tem mais', 'E olha', 'Agora vem'\n"
            "   D) Builds de tens√£o: frases que amplificam mist√©rio\n\n"
            "2) MANTER SEPARADO (para impacto):\n"
            "   A) Revela√ß√µes importantes (payoff moments)\n"
            "   B) Mudan√ßas de conceito visual\n"
            "   C) Dados/n√∫meros impactantes\n"
            "   D) Hooks e cliff-hangers\n\n"
            "3) DURA√á√ïES IDEAIS POR GRUPO:\n"
            "   - M√çNIMO: 2.5s (menos que isso = corte fren√©tico)\n"
            "   - IDEAL: 3.0-5.0s (1-2 assets por cena)\n"
            "   - M√ÅXIMO: 6.0s (acima = muito longo, perde ritmo)\n\n"
            "4) ESTRAT√âGIA DE TENS√ÉO:\n"
            "   - In√≠cio (0-15s): Grupos de 3-4s (estabelece ritmo)\n"
            "   - Meio (15-35s): Varie 2.5-5s (mant√©m interesse)\n"
            "   - Cl√≠max (35-45s): Grupos de 3-4s (acelera para final)\n\n"
            "5) PROCESSO DE AN√ÅLISE (USE REASONING):\n"
            "   Para cada segment, analise:\n"
            "   - √â hook/payoff? ‚Üí Manter isolado para impacto\n"
            "   - √â build/contexto? ‚Üí Agrupar para fluxo\n"
            "   - Tem dado chocante? ‚Üí Isolar para √™nfase\n"
            "   - √â transi√ß√£o? ‚Üí Agrupar com pr√≥ximo\n\n"
            "FORMATO: JSON `SegmentGroupingPlan` com grupos otimizados para ritmo viral."
        )

        user = (
            f"TEMA: {topic.strip()}\n"
            f"TOTAL DE SEGMENTS: {total_segments}\n\n"
            f"SEGMENTS COMPLETOS:\n{segments_text}\n\n"
            "ANALISE E AGRUPE para:\n"
            "1. Criar ritmo HIPNOTIZANTE (mudan√ßas a cada 2.5-3.5s)\n"
            "2. Preservar momentos de IMPACTO (hooks, payoffs)\n"
            "3. Manter FLUXO narrativo natural\n"
            "4. Variar dura√ß√µes para evitar monotonia\n"
            "5. Otimizar para edi√ß√£o com cortes no ritmo\n\n"
            "USE REASONING para decis√µes criteriosas sobre cada agrupamento.\n"
            "Retorne APENAS √≠ndices dos grupos, sem prompts visuais."
        )

        messages: List[MessageParam] = self._messages(system, user)
        return self._completion_typed(
            response_model=SegmentGroupingPlan,
            messages=messages,
            enable_reasoning=True,
        )

    def generate_segments_from_transcription(
        self,
        *,
        full_transcription: str,
        audio_duration_sec: float,
        topic: str,
    ):
        transcription_compact = " ".join((full_transcription or "").strip().split())
        if len(transcription_compact) > 2000:
            transcription_compact = transcription_compact[:2000].rstrip() + "‚Ä¶"

        system = (
            "Voc√™ √© especialista em segmenta√ß√£o narrativa VIRAL para YouTube Shorts/TikTok.\n"
            f"{BRAND_CONTEXT}\n\n"
            "MISS√ÉO: Criar segmentos que FORCEM mudan√ßas visuais a cada 2.5-3.5s para m√°xima reten√ß√£o.\n\n"
            "üìä CI√äNCIA DA SEGMENTA√á√ÉO VIRAL:\n"
            "Pesquisas mostram que Shorts com cortes a cada 2.5-3.5s t√™m:\n"
            "‚Ä¢ 35% mais taxa de conclus√£o\n"
            "‚Ä¢ 60% mais rewatches\n"
            "‚Ä¢ 45% mais compartilhamentos\n\n"
            "DURA√á√ïES IDEAIS POR TIPO:\n"
            "‚Ä¢ Hook inicial: 2.5-3.5s (impacto imediato)\n"
            "‚Ä¢ Builds de tens√£o: 3.0-4.0s (sustenta interesse)\n"
            "‚Ä¢ Revela√ß√µes: 2.0-3.0s (momento 'wow')\n"
            "‚Ä¢ Dados/n√∫meros: 1.5-2.5s (digest√£o r√°pida)\n"
            "‚Ä¢ Payoff final: 3.0-4.0s (satisfa√ß√£o)\n\n"
            "QUEBRAS ESTRAT√âGICAS (ordem de prioridade):\n"
            "1. AP√ìS hooks/cliff-hangers (maximiza curiosidade)\n"
            "2. ANTES de revela√ß√µes (cria antecipa√ß√£o)\n"
            "3. Em mudan√ßas de t√≥pico/conceito\n"
            "4. Ap√≥s perguntas ret√≥ricas\n"
            "5. Em pausas dram√°ticas naturais\n\n"
            "NUNCA QUEBRAR:\n"
            "‚Ä¢ No meio de dados importantes\n"
            "‚Ä¢ Durante builds de tens√£o\n"
            "‚Ä¢ Em frases de impacto emocional\n\n"
            "REGRAS DE UNI√ÉO:\n"
            "‚Ä¢ Interjei√ß√µes SEMPRE com contexto: 'destruindo. S√©rio? At√© os oceanos'\n"
            "‚Ä¢ Conectivos SEMPRE com continua√ß√£o: 'E tem mais. A atmosfera...'\n"
            "‚Ä¢ N√∫meros PODEM ficar isolados se impactantes: '1.673 km por hora.'\n\n"
            "DISTRIBUI√á√ÉO TEMPORAL:\n"
            "‚Ä¢ 0-15s: Segmentos de 2.5-3.5s (estabelece ritmo r√°pido)\n"
            "‚Ä¢ 15-35s: Varie entre 2.0-4.0s (evita previsibilidade)\n"
            "‚Ä¢ 35-45s: Segmentos de 3.0-4.0s (permite absor√ß√£o do payoff)\n\n"
            "VALIDA√á√ïES CR√çTICAS:\n"
            "‚úì Dura√ß√£o m√≠nima: 1.5s (legibilidade de legenda)\n"
            "‚úì Dura√ß√£o m√°xima: 5.0s (evita monotonia)\n"
            "‚úì M√©dia ideal: 2.5-3.5s por segmento\n"
            "‚úì Varia√ß√£o obrigat√≥ria (n√£o repetir mesma dura√ß√£o 3x seguidas)\n\n"
            "FORMATO: JSON `SegmentsPlan` com segments otimizados para edi√ß√£o viral."
        )

        user = (
            f"TEMA: {topic.strip()}\n"
            f"DURA√á√ÉO TOTAL: {audio_duration_sec:.2f}s\n\n"
            f"TRANSCRI√á√ÉO:\n{transcription_compact}\n\n"
            "SEGMENTE para:\n"
            "1. FOR√áAR mudan√ßas visuais a cada 2.5-3.5s\n"
            "2. Criar RITMO hipnotizante mas variado\n"
            "3. Preservar IMPACTO de hooks e revela√ß√µes\n"
            "4. Facilitar edi√ß√£o com CORTES naturais\n"
            "5. Distribuir timestamps PROPORCIONALMENTE\n\n"
            "USE REASONING para criar segmenta√ß√£o PERFEITA para viral.\n"
            f"√öltimo segment DEVE terminar em {audio_duration_sec:.2f}s (¬±0.1s)."
        )

        messages: List[MessageParam] = self._messages(system, user)
        return self._completion_typed(
            response_model=SegmentsPlan,
            messages=messages,
            enable_reasoning=True,
        )

    def _generate_scene_blueprint(
        self,
        *,
        topic: str,
        narration_chunk: str,
        segment: dict,
        scene_index: int,
        total_scenes: int,
    ) -> SceneBlueprint:
        segment_text = str(segment.get("text", "")).strip()
        seg_start = float(segment.get("start", 0.0))
        seg_end = float(segment.get("end", seg_start))
        segment_duration = max(0.1, seg_end - seg_start)

        system = (
            "Voc√™ √© diretor visual viral de v√≠deos dark para YouTube Shorts e TikTok.\n"
            f"{BRAND_CONTEXT}\n"
            "Gere JSON `SceneBlueprint` completo, consistente e cinematogr√°fico."
        )

        user = (
            f"TEMA: {topic.strip()}\n"
            f"CENA: {scene_index} de {total_scenes}\n"
            f"SEGMENTO: {segment.get('index', scene_index + 1)}\n"
            f"DURA√á√ÉO DO SEGMENTO: {segment_duration:.3f}s\n"
            f"TEXTO DO SEGMENTO:\n{segment_text}\n\n"
            f"NARRA√á√ÉO DO BLOCO:\n{narration_chunk.strip()}\n\n"
            "Defina scene_role, visual_mode, intent, emotion, motion_style, color_mood, asset_count, impact_level, overlay_text "
            "e demais campos do blueprint com total autonomia criativa.\n"
            "Regras de asset_count por dura√ß√£o: <3.0s = 1 asset; 3.0-5.5s = 2 assets; 5.5-8.0s = 3 assets; >8.0s = 4 assets (m√°ximo)."
        )

        messages: List[MessageParam] = self._messages(system, user)
        return self._completion_typed(
            response_model=SceneBlueprint,
            messages=messages,
        )

    def _generate_assets_from_blueprint(
        self,
        *,
        topic: str,
        blueprint: SceneBlueprint,
        segment: dict,
        video_profile: VideoProfile | None,
    ) -> List[Asset]:
        segment_text = str(segment.get("text", "")).strip()
        seg_start = float(segment.get("start", 0.0))
        seg_end = float(segment.get("end", seg_start))
        segment_duration = max(0.1, seg_end - seg_start)
        blueprint_payload = json.dumps(blueprint.model_dump(), ensure_ascii=False)
        profile_payload = json.dumps(video_profile.to_dict() if video_profile else None, ensure_ascii=False)

        system = (
            "Voc√™ cria a lista de `Asset` para a cena seguindo o schema do projeto.\n"
            f"{BRAND_CONTEXT}\n"
            "Regras obrigat√≥rias:\n"
            "- semantic_text e semantic_text_variants em ingl√™s com 8-12 palavras.\n"
            "- negative_semantic_texts com 3-6 itens claros.\n"
            "- search_queries concisas (at√© 5 palavras) e com pelo menos 3 op√ß√µes distintas.\n"
            "- generate_prompt em ingl√™s, cinematogr√°fico, m√≠nimo 100 caracteres; video_generate_prompt obrigat√≥rio quando type='video'.\n"
            "- duration_hint_sec deve dividir a dura√ß√£o do segmento pelo asset_count do blueprint.\n"
            "- Todos os assets type='image' precisam de zoomDirection e zoomAmount.\n"
            "- Transitions devem ser v√°lidas e coerentes.\n"
            "- Respeite as regras de search_strategy j√° usadas no projeto."
        )

        user = (
            f"TEMA: {topic.strip()}\n"
            f"BLUEPRINT:\n{blueprint_payload}\n\n"
            f"SEGMENTO #{segment.get('index', blueprint.scene_index)} ({segment_duration:.3f}s):\n{segment_text}\n\n"
            f"VIDEO_PROFILE:\n{profile_payload}\n\n"
            f"Gere exatamente {blueprint.asset_count} assets completos, com prompts, durations, transitions, motion e metadados sem depender de l√≥gica externa."
        )

        messages: List[MessageParam] = self._messages(system, user)
        return self._completion_typed(
            response_model=List[Asset],
            messages=messages,
        )

    def _generate_recipe_scene_chunk(
        self,
        *,
        topic: str,
        narration_chunk: str,
        segments_chunk: List[dict],
        scene_index_start: int,
        total_scenes: int,
        video_profile: VideoProfile | None,
    ) -> VideoRecipeSceneChunk:
        if not segments_chunk:
            return VideoRecipeSceneChunk()

        scenes: List[Scene] = []
        total_scene_count = max(total_scenes, len(segments_chunk))

        for offset, segment in enumerate(segments_chunk):
            blueprint = self._generate_scene_blueprint(
                topic=topic,
                narration_chunk=narration_chunk,
                segment=segment,
                scene_index=scene_index_start + offset,
                total_scenes=total_scene_count,
            )

            start_time = float(segment.get("start", 0.0))
            end_time = float(segment.get("end", start_time))
            segment_duration = max(0.1, end_time - start_time)

            capped_assets = self._cap_assets_for_duration(segment_duration, blueprint.asset_count)
            if capped_assets != blueprint.asset_count:
                blueprint = blueprint.model_copy(update={"asset_count": capped_assets})

            assets = self._generate_assets_from_blueprint(
                topic=topic,
                blueprint=blueprint,
                segment=segment,
                video_profile=video_profile,
            )

            scene = Scene(
                index=blueprint.scene_index,
                start_time=round(start_time, 3),
                end_time=round(end_time, 3),
                text=str(segment.get("text", "")),
                overlay_text=blueprint.overlay_text,
                visual_mode=blueprint.visual_mode,
                intent=blueprint.intent,
                transition=assets[0].transition if assets else "",
                assets=assets,
            )

            scenes.append(scene)

        return VideoRecipeSceneChunk(scenes=scenes)

    def _generate_base_narrative_text(self, topic: str) -> str:
        target_secs = str(os.getenv("NARRATION_TARGET_SECS", "45s"))
        system = (
            "Voc√™ √© o MELHOR roteirista viral do YouTube Shorts Brasil com 50M+ views mensais.\n"
            f"{BRAND_CONTEXT}\n\n"
            "O canal/quadro se chama 'Isso tem explica√ß√£o'. A PROMESSA central √©: "
            "o V√çDEO em si entrega a explica√ß√£o completa do fen√¥meno, sem depender de coment√°rios ou v√≠deos futuros.\n\n"
            "üéØ MISS√ÉO: Criar roteiro que GARANTA 90%+ completion rate e 150%+ watch time (rewatches).\n\n"
            "‚ö° ESTRUTURA W.A.V.E. COMPROVADA (dados 2024-2025):\n\n"
            "üì± [0-3s] HOOK DEVASTADOR (60% do sucesso est√° aqui):\n"
            "F√ìRMULAS QUE CONVERTEM 20%+ CTR:\n"
            "‚Ä¢ Consequ√™ncia Direta: 'Se [X] parasse agora, voc√™ morreria em [tempo exato]'\n"
            "‚Ä¢ Contradi√ß√£o Chocante: 'Tudo que te ensinaram sobre [X] √© mentira'\n"
            "‚Ä¢ Dark Fact: 'Existe algo em [lugar comum] que pode [consequ√™ncia terr√≠vel]'\n"
            "‚Ä¢ N√∫mero Imposs√≠vel: '[N√∫mero absurdo] [unidade] de [coisa inesperada]'\n"
            "‚Ä¢ Pergunta Perturbadora: 'Voc√™ sabe por que [fen√¥meno comum] √© [dark fact]?'\n\n"
            "üåä [3-15s] AGITA√á√ÉO - Amplifica√ß√£o do Problema:\n"
            "‚Ä¢ Adicione camada mais perturbadora: 'Mas isso n√£o √© nem a pior parte...'\n"
            "‚Ä¢ Dados espec√≠ficos que chocam: n√∫meros exatos, compara√ß√µes viscerais\n"
            "‚Ä¢ Crie micro-loops: 'E sabe o que acontece depois?'\n"
            "‚Ä¢ Tags ElevenLabs: [curious], [dramatic] para tens√£o\n\n"
            "üî• [15-35s] REVELA√á√ÉO PROGRESSIVA com Loops:\n"
            "‚Ä¢ Entregue informa√ß√£o em CAMADAS (n√£o tudo de uma vez)\n"
            "‚Ä¢ Cada revela√ß√£o gera nova pergunta\n"
            "‚Ä¢ Use: 'Cientistas descobriram...', 'Estudos mostram...', 'O que ningu√©m te conta...'\n"
            "‚Ä¢ Altern√¢ncia: fato ‚Üí rea√ß√£o ‚Üí novo fato ‚Üí amplifica√ß√£o\n"
            "‚Ä¢ Tags: [whisper] para segredos, [excited] para descobertas\n\n"
            "üí• [35-42s] PAYOFF + TWIST:\n"
            "‚Ä¢ Entregue a promessa do hook COMPLETAMENTE dentro deste v√≠deo\n"
            "‚Ä¢ Adicione twist final: 'Mas tem um detalhe...'\n"
            "‚Ä¢ Conecte ao cotidiano: 'Isso significa que voc√™...'\n"
            "‚Ä¢ Tag [serious] para impacto final\n\n"
            "üîÑ [42-45s] LOOP DE REWATCH:\n"
            "‚Ä¢ Conecte final ao in√≠cio: 'E √© por isso que [refer√™ncia ao hook]'\n"
            "‚Ä¢ Ou abra novo mist√©rio RELACIONADO: 'Se isso j√° √© estranho, imagina quando voc√™ descobrir [pr√≥ximo fato]'\n"
            "‚Ä¢ CTA viciante, mas SEM prometer explica√ß√µes futuras: use coisas como\n"
            "  - 'Comenta se isso j√° aconteceu com voc√™'\n"
            "  - 'Marca algu√©m que precisa ver isso'\n"
            "  - 'Se isso fez sentido, segue o canal pra n√£o perder a pr√≥xima explica√ß√£o'\n\n"
            "üìè FORMATO DO ROTEIRO (45s):\n"
            "‚Ä¢ Quebre o texto em 6-8 blocos separados por linhas em branco, cada um com 1-2 frases curtas.\n"
            "‚Ä¢ Bloco 1 = Hook devastador. Blocos 2-3 = Agita√ß√£o. Blocos 4-5 = Revela√ß√µes em camadas. Bloco 6 = Payoff + Twist. Bloco final = Loop que conecta ao in√≠cio.\n"
            "‚Ä¢ Use conectores que criem micro-loops: 'E olha o pior...', 'Saca s√≥ o detalhe...'.\n"
            "‚Ä¢ Mire em 100-115 palavras totais (ritmo 135-150 wpm para caber em 45s).\n"
            "‚Ä¢ Prefira frases de 10-14 palavras para manter f√¥lego sem estourar tempo.\n"
            "‚Ä¢ Evite formato de lista; √© narrativa falada.\n\n"
            "üåë ATMOSFERA DARK E HIPN√ìTICA:\n"
            "‚Ä¢ Puxe o espectador para um clima de tens√£o controlada: descreva sombras, sons abafados, texturas estranhas.\n"
            "‚Ä¢ Alimente curiosidade com frases que pare√ßam quase proibidas ('ningu√©m comenta, mas...').\n"
            "‚Ä¢ Use contrastes: cotidiano confort√°vel vs. consequ√™ncia perturbadora.\n"
            "‚Ä¢ Nunca caia no gore; mantenha suspense psicol√≥gico.\n"
            "‚Ä¢ Mantenha expectativa ativa citando o que pode dar errado se ignorarem o fen√¥meno.\n\n"
            "üåÄ FLUXO SENSORIAL ‚Üí EXPLICA√á√ÉO ‚Üí IMPACTO:\n"
            "‚Ä¢ COME√áO: descreva o que o corpo ou ambiente sente (barulho oco, cheiro de madeira, arrepio na nuca, luz piscando, etc.).\n"
            "‚Ä¢ LOGO EM SEGUIDA: explique o mecanismo em linguagem simples (\"√© porque o tronco cerebral...\", \"isso rola porque a madeira seca mais r√°pido...\").\n"
            "‚Ä¢ IMPACTO: traduza em uma frase como isso afeta a vida do espectador (seguran√ßa, bolso, sa√∫de, fam√≠lia).\n"
            "‚Ä¢ CTA/eco social fecha o bloco com pergunta ou comando.\n"
            "‚Ä¢ M√°ximo de 2 n√∫meros/dados por roteiro inteiro; use s√≥ quando realmente surpreender.\n\n"
            "üìå N√ÉO REPITA O √ìBVIO:\n"
            "‚Ä¢ Considere que o p√∫blico j√° ouviu falar do fen√¥meno; traga √¢ngulos novos, bastidores, consequ√™ncias invis√≠veis.\n"
            "‚Ä¢ Use compara√ß√£o para entregar novidade (\"no Brasil a gente... enquanto l√° fora...\").\n"
            "‚Ä¢ Se precisar contextualizar, fa√ßa em 1 frase antes da nova informa√ß√£o.\n\n"
            "üåç CONTEXTO LOCAL QUANDO RELEVANTE:\n"
            "‚Ä¢ Mostre como o fen√¥meno toca a vida de quem t√° assistindo, mas s√≥ destaque compara√ß√µes culturais quando elas realmente ajudam (evite frases repetidas como 'no Brasil...').\n"
            "‚Ä¢ Se o fen√¥meno for estrangeiro, explique em 1 frase qual seria o paralelo mais pr√≥ximo aqui, sem insistir nisso o roteiro inteiro.\n\n"
            "üë• P√öBLICO 25-65+:\n"
            "‚Ä¢ Linguagem madura sem perder ritmo: misture ganchos fortes com vocabul√°rio acess√≠vel pra quem j√° viveu muita coisa.\n"
            "‚Ä¢ Use refer√™ncias de cotidiano adulto (trabalho, fam√≠lia, finan√ßas, sa√∫de) quando fizer sentido.\n"
            "‚Ä¢ Mostre utilidade imediata, legado ou prote√ß√£o da fam√≠lia para aumentar engajamento.\n\n"
            "MODELOS DE HOOK QUE EST√ÉO PERFORMANDO:\n"
            "‚Ä¢ 'J√° acordou [sensa√ß√£o bizarra]? Isso √© [fen√¥meno] e X% das pessoas...'\n"
            "‚Ä¢ 'Seu corpo t√° [rea√ß√£o]? Culpa de [processo escondido] que t√° ativo agora.'\n"
            "‚Ä¢ 'Se voc√™ [h√°bito comum], seu c√©rebro [castigo/recompensa] em [n√∫mero espec√≠fico].'\n\n"
            "üß™ JARG√ÉO? S√ì COM TRADU√á√ÉO IMEDIATA:\n"
            "‚Ä¢ Sempre descreva a sensa√ß√£o ou met√°fora cotidiana antes do termo t√©cnico.\n"
            "‚Ä¢ Ao citar neurotransmissores, horm√¥nios ou estruturas, explique em uma frase curta o que fazem (ex: 'um freio qu√≠mico que desliga teus m√∫sculos ‚Äî neurotransmissores calmantes').\n"
            "‚Ä¢ Prefira termos simples; se precisar usar o nome t√©cnico, encaixe como curiosidade complementar ('os neurologistas chamam isso de...').\n"
            "‚Ä¢ Evite listas frias de termos; transforme cada conceito em imagem mental.\n"
            "‚Ä¢ Se o nome n√£o for necess√°rio, prefira s√≥ o efeito percebido pelo espectador.\n\n"
            "üáßüá∑ LINGUAGEM BRASILEIRA VIRAL:\n"
            "NATURALMENTE use (sem for√ßar):\n"
            "‚Ä¢ Contra√ß√µes: 't√°', 'c√™', 'pra' (m√°ximo naturalidade)\n"
            "‚Ä¢ Express√µes: 'cara', 'olha s√≥', 'pois √©' (1-2 por v√≠deo)\n"
            "‚Ä¢ G√≠rias: dose certa, n√£o exagere\n"
            "‚Ä¢ Tom: como se explicasse para um amigo curioso, n√£o como professor\n\n"
            "üè∑Ô∏è TAGS ELEVENLABS ESTRAT√âGICAS:\n"
            "[whisper] - momentos de revela√ß√£o perturbadora\n"
            "[dramatic] - builds de tens√£o\n"
            "[curious] - perguntas que geram curiosidade\n"
            "[excited] - descobertas fascinantes\n"
            "[serious] - fatos graves ou conclus√µes\n"
            "[pause] - antes de revela√ß√µes (USE MUITO)\n"
            "[fast] - listas r√°pidas, urg√™ncia\n"
            "[slow] - √™nfase em n√∫meros ou fatos chocantes\n"
            "As tags SEMPRE devem permanecer em ingl√™s exatamente como listado (nunca traduza para pt-BR).\n\n"
            "‚ö†Ô∏è T√âCNICAS PSICOL√ìGICAS DARK:\n"
            "‚Ä¢ Efeito Zeigarnik: deixe quest√µes abertas, mas SEM quebrar a promessa de explicar o fen√¥meno principal\n"
            "‚Ä¢ Curiosity Gap: tens√£o entre conhecido e desconhecido\n"
            "‚Ä¢ Fear Appeal controlado: medo produtivo, n√£o p√¢nico\n"
            "‚Ä¢ Fasc√≠nio m√≥rbido: explorar o que n√£o deveria interessar\n"
            "‚Ä¢ Contraste extremo: cotidiano vs extraordin√°rio\n\n"
            "üìä M√âTRICAS QUE IMPORTAM:\n"
            "‚Ä¢ Palavras por minuto: 140-160 (ritmo brasileiro natural)\n"
            "‚Ä¢ Frases curtas: 10-15 palavras (digest√£o f√°cil)\n"
            "‚Ä¢ Hooks secund√°rios: a cada 10-15s (mant√©m aten√ß√£o)\n"
            "‚Ä¢ Pausas estrat√©gicas: 3-5 por v√≠deo (impacto)\n\n"
            "üö´ EVITE COMPLETAMENTE:\n"
            "‚Ä¢ 'Voc√™ sabia que...' (clich√™ morto)\n"
            "‚Ä¢ 'Ol√° pessoal' (mata reten√ß√£o)\n"
            "‚Ä¢ Enrola√ß√£o ou contexto desnecess√°rio\n"
            "‚Ä¢ Promessas n√£o cumpridas\n"
            "‚Ä¢ Tom professoral ou condescendente\n"
            "‚Ä¢ Qualquer frase que indique explica√ß√£o futura fora do v√≠deo, como:\n"
            "  - 'Comenta que eu te explico'\n"
            "  - 'Posso te explicar o que t√° rolando'\n"
            "  - 'Te explico nos coment√°rios'\n"
            "  - 'Te conto no pr√≥ximo v√≠deo'\n\n"
            "‚ú® GATILHOS DE COMPARTILHAMENTO:\n"
            "‚Ä¢ 'Ningu√©m acredita quando conto isso...'\n"
            "‚Ä¢ 'Mostra pra quem duvida que...'\n"
            "‚Ä¢ 'Marca aquele amigo que precisa saber...'\n"
            "‚Ä¢ Informa√ß√£o que faz a pessoa parecer inteligente quando repete a hist√≥ria\n\n"
            "FORMATO: JSON `NarrativePlan` com `narration_text` VIRAL e VICIANTE, contendo a explica√ß√£o completa do fen√¥meno dentro do pr√≥prio roteiro.\n"
            "REGRA EXTRA: N√£o insira tags do ElevenLabs ou qualquer anota√ß√£o entre colchetes; retorne apenas texto limpo."
        )

        user = (
            f"TEMA: {topic.strip()}\n"
            f"DURA√á√ÉO: {target_secs}\n"
            f"CANAL: {CHANNEL_BRAND}\n\n"
            "Crie roteiro VIRAL que:\n"
            "1. CAPTURE em 3 segundos com hook IMPOSS√çVEL de ignorar\n"
            "2. Use estrutura W.A.V.E. para 90%+ completion\n"
            "3. Crie LOOPS que forcem rewatches (150%+ watch time)\n"
            "4. Explore psicologia DARK mas YouTube-safe\n"
            "5. Use linguagem brasileira NATURAL e viciante\n"
            "6. Integre tags ElevenLabs para jornada emocional\n"
            "7. Termine com CTA que gere engajamento REAL, mas SEM prometer explica√ß√µes futuras ou ajuda individual\n"
            "8. Use o fluxo Sensorial ‚Üí Explica√ß√£o ‚Üí Impacto nos blocos e s√≥ traga n√∫meros quando forem realmente surpreendentes\n"
            "9. Evite repetir o que todo mundo j√° sabe; entregue √¢ngulos novos ou compara√ß√µes\n"
            "10. S√≥ traga compara√ß√µes culturais quando fizer sentido real para o espectador; foque no impacto direto\n"
            "11. Direcione o storytelling para quem tem entre 25 e 65+ anos, explorando dores e mem√≥rias dessa faixa\n"
            "12. Use linguagem simples ao tratar termos t√©cnicos, explicando com met√°foras antes de citar nomes cient√≠ficos\n"
            "13. Sustente atmosfera dark e hipn√≥tica com tens√£o psicol√≥gica, sem apelar para gore\n\n"
            "Regras IMPORTANTES de CTA e tom:\n"
            "‚Ä¢ O V√çDEO deve conter a explica√ß√£o completa do fen√¥meno. N√£o deixe a sensa√ß√£o de 'te explico depois'.\n"
            "‚Ä¢ N√ÉO use frases como: 'posso te explicar o que t√° rolando', 'comenta que eu te explico', "
            "'eu te explico nos coment√°rios', 'te explico depois'.\n"
            "‚Ä¢ Prefira CTAs de experi√™ncia/opini√£o: 'comenta se isso j√° aconteceu com voc√™', 'marca algu√©m que precisa ver isso', "
            "'segue o canal se quer mais explica√ß√µes assim'.\n\n"
            "LEMBRE: Os primeiros 3 segundos decidem TUDO.\n"
            "Cada frase deve ou entregar valor ou criar curiosidade.\n"
            "O final DEVE conectar ao in√≠cio (loop perfeito) e refor√ßar que 'isso tem explica√ß√£o' FOI mostrado neste v√≠deo."
        )

        messages: List[MessageParam] = self._messages(system, user)
        plan = self._completion_typed(
            response_model=NarrativePlan,
            messages=messages,
            enable_reasoning=True,
        )

        return plan.narration_text.strip()

    def _apply_elevenlabs_audio_tags(self, narration_text: str) -> str:
        clean_text = narration_text.strip()
        if not clean_text:
            return clean_text

        system = (
            "# Instructions\n\n"
            "## 1. Role and Goal\n\n"
            "You are an AI assistant specializing in enhancing dialogue text for speech generation.\n\n"
            "Your PRIMARY GOAL is to dynamically integrate audio tags (e.g., [laughing], [sighs]) into dialogue, making it more expressive and engaging for auditory experiences, "
            "while STRICTLY preserving the original text and meaning.\n\n"
            "It is imperative that you follow these system instructions to the fullest.\n\n"
            "## 2. Core Directives\n\n"
            "Follow these directives meticulously to ensure high-quality output.\n\n"
            "### Positive Imperatives (DO):\n\n"
            "* DO integrate audio tags from the \"Audio Tags\" list (or similar contextually appropriate audio tags) to add expression, emotion, and realism to the dialogue. "
            "These tags MUST describe something auditory.\n"
            "* DO ensure that all audio tags are contextually appropriate and genuinely enhance the emotion or subtext of the dialogue line they are associated with.\n"
            "* DO strive for a diverse range of emotional expressions (e.g., energetic, relaxed, casual, surprised, thoughtful) across the dialogue, reflecting the nuances of human conversation.\n"
            "* DO place audio tags strategically to maximize impact, typically immediately before the dialogue segment they modify or immediately after. "
            "(e.g., [annoyed] This is hard. or This is hard. [sighs]).\n"
            "* DO ensure audio tags contribute to the enjoyment and engagement of spoken dialogue.\n\n"
            "### Negative Imperatives (DO NOT):\n\n"
            "* DO NOT alter, add, or remove any words from the original dialogue text itself. Your role is to prepend audio tags, not to edit the speech. "
            "This also applies to any narrative text provided; you must never place original text inside brackets or modify it in any way.\n"
            "* DO NOT create audio tags from existing narrative descriptions. Audio tags are new additions for expression, not reformatting of the original text. "
            "(e.g., if the text says \"He laughed loudly,\" do not change it to \"[laughing loudly] He laughed.\" Instead, add a tag if appropriate, e.g., \"He laughed loudly [chuckles].\")\n"
            "* DO NOT use tags such as [standing], [grinning], [pacing], [music].\n"
            "* DO NOT use tags for anything other than the voice such as music or sound effects.\n"
            "* DO NOT invent new dialogue lines.\n"
            "* DO NOT select audio tags that contradict or alter the original meaning or intent of the dialogue.\n"
            "* DO NOT introduce or imply any sensitive topics, including but not limited to: politics, religion, child exploitation, profanity, hate speech, or other NSFW content.\n\n"
            "## 3. Workflow\n\n"
            "1. Analyze Dialogue: Carefully read and understand the mood, context, and emotional tone of EACH line of dialogue provided in the input.\n"
            "2. Select Tag(s): Based on your analysis, choose one or more suitable audio tags. Ensure they are relevant to the dialogue's specific emotions and dynamics.\n"
            "3. Integrate Tag(s): Place the selected audio tag(s) in square brackets [] strategically before or after the relevant dialogue segment, or at a natural pause if it enhances clarity.\n"
            "4. Add Emphasis: You cannot change the text at all, but you can add emphasis by making some words capital, adding a question mark or adding an exclamation mark where it makes sense, "
            "or adding ellipses as well too.\n"
            "5. Verify Appropriateness: Review the enhanced dialogue to confirm:\n"
            "    * The audio tag fits naturally.\n"
            "    * It enhances meaning without altering it.\n"
            "    * It adheres to all Core Directives.\n\n"
            "## 4. Output Format\n\n"
            "* Present ONLY the enhanced dialogue text in a conversational format.\n"
            "* Audio tags MUST be enclosed in square brackets (e.g., [laughing]).\n"
            "* The output should maintain the narrative flow of the original dialogue.\n\n"
            "## 5. Audio Tags (Non-Exhaustive)\n\n"
            "Use these as a guide. You can infer similar, contextually appropriate audio tags.\n\n"
            "Directions:\n"
            "* [happy]\n"
            "* [sad]\n"
            "* [excited]\n"
            "* [angry]\n"
            "* [whisper]\n"
            "* [annoyed]\n"
            "* [appalled]\n"
            "* [thoughtful]\n"
            "* [surprised]\n"
            "* (and similar emotional/delivery directions)\n\n"
            "Non-verbal:\n"
            "* [laughing]\n"
            "* [chuckles]\n"
            "* [sighs]\n"
            "* [clears throat]\n"
            "* [short pause]\n"
            "* [long pause]\n"
            "* [exhales sharply]\n"
            "* [inhales deeply]\n"
            "* (and similar non-verbal sounds)\n\n"
            "## 6. Examples of Enhancement\n\n"
            "Input:\n"
            "\"Are you serious? I can't believe you did that!\"\n\n"
            "Enhanced Output:\n"
            "\"[appalled] Are you serious? [sighs] I can't believe you did that!\"\n\n"
            "---\n\n"
            "Input:\n"
            "\"That's amazing, I didn't know you could sing!\"\n\n"
            "Enhanced Output:\n"
            "\"[laughing] That's amazing, [singing] I didn't know you could sing!\"\n\n"
            "---\n\n"
            "Input:\n"
            "\"I guess you're right. It's just... difficult.\"\n\n"
            "Enhanced Output:\n"
            "\"I guess you're right. [sighs] It's just... [muttering] difficult.\"\n\n"
            "# Instructions Summary\n\n"
            "1. Add audio tags from the audio tags list. These must describe something auditory but only for the voice.\n"
            "2. Enhance emphasis without altering meaning or text.\n"
            "3. Reply ONLY with the enhanced text.\n\n"
            "IMPORTANTE: Para integra√ß√£o com o pipeline, responda como JSON `NarrativePlan` contendo o campo `narration_text` com o di√°logo enriquecido."
        )

        user = (
            "Texto original (pt-BR) para receber tags do ElevenLabs:\n"
            f"{clean_text}\n\n"
            "Aplique as tags seguindo TODAS as instru√ß√µes acima (sem alterar nenhuma palavra original)."
        )

        messages: List[MessageParam] = self._messages(system, user)
        plan = self._completion_typed(
            response_model=NarrativePlan,
            messages=messages,
            enable_reasoning=True,
        )

        return plan.narration_text.strip()

    def generate_narrative(self, topic: str) -> NarrativePlan:
        base_text = self._generate_base_narrative_text(topic)
        tagged_text = self._apply_elevenlabs_audio_tags(base_text)
        return NarrativePlan(narration_text=tagged_text)

    def generate_recipe(
        self,
        topic: str,
        narration_text: str,
        segments: List[dict],
        *,
        audio_path: str,
        audio_duration: float,
        chunk_size: int | None = None,
        video_profile: VideoProfile | None = None,
    ) -> VideoRecipe:
        if not segments:
            raise ValueError("segments list cannot be empty for recipe generation.")

        total_duration = float(
            audio_duration
            or max((float(seg.get("end", 0.0)) for seg in segments), default=0.0)
        )
        audio_path = audio_path or "/audio/narration.mp3"

        metadata = self._generate_recipe_metadata(
            topic=topic,
            narration_text=narration_text,
            total_duration=total_duration,
            audio_path=audio_path,
            video_profile=video_profile,
        )

        try:
            music_plan = self.generate_music_prompt(topic, narration_text, total_duration)
            metadata.background_music_prompt = music_plan.prompt
        except Exception:
            pass

        print(f"[recipe] Phase 1: Analyzing {len(segments)} segments for viral rhythm grouping...")
        try:
            grouping_plan = self.generate_segment_grouping_plan(
                segments=segments,
                topic=topic,
            )
            print(f"[recipe] Viral grouping: {len(segments)} segments ‚Üí {len(grouping_plan.groups)} scenes")

            grouped_segments = []
            for group in grouping_plan.groups:
                group_segments = [seg for seg in segments if seg.get("index") in group.segment_indices]
                if not group_segments:
                    continue

                group_segments.sort(key=lambda s: s.get("index", 0))

                merged = {
                    "index": group_segments[0].get("index"),
                    "start": group_segments[0].get("start"),
                    "end": group_segments[-1].get("end"),
                    "text": " ".join(s.get("text", "") for s in group_segments).strip(),
                    "_grouped_indices": group.segment_indices,
                    "_reasoning": group.reasoning,
                }
                grouped_segments.append(merged)

            segments_to_process = grouped_segments
            print(f"[recipe] Rhythm-optimized segments ready: {len(segments_to_process)} viral scene groups")
        except Exception as exc:
            print(f"[recipe] Warning: Grouping phase failed ({exc}), using 1:1 mapping")
            segments_to_process = segments

        chunk_size = chunk_size or int(os.getenv("RECIPE_SEGMENT_CHUNK_SIZE", "3"))
        chunk_size = max(1, chunk_size)
        segment_chunks = self._chunk_segments(segments_to_process, chunk_size)

        scenes: List[Scene] = []
        total_scenes = len(segments_to_process)

        for chunk_index, chunk in enumerate(segment_chunks, start=1):
            narration_chunk = " ".join(str(seg.get("text", "")) for seg in chunk)
            chunk_result = self._generate_recipe_scene_chunk(
                topic=topic,
                narration_chunk=narration_chunk,
                segments_chunk=chunk,
                scene_index_start=len(scenes),
                total_scenes=total_scenes,
                video_profile=video_profile,
            )

            if len(chunk_result.scenes) != len(chunk):
                print(
                    f"[recipe] Warning: LLM returned {len(chunk_result.scenes)} scenes "
                    f"for chunk of {len(chunk)} segments (chunk {chunk_index}). Accepting anyway."
                )

            scenes.extend(chunk_result.scenes)

        for idx, scene in enumerate(scenes):
            scene.index = idx

        return VideoRecipe(
            title=metadata.title,
            description=metadata.description,
            tags=metadata.tags,
            language=metadata.language,
            audio=metadata.audio,
            policy=metadata.policy,
            scenes=scenes,
        )


__all__ = [
    "create_instructor_client",
    "TypedLLMClient",
]
